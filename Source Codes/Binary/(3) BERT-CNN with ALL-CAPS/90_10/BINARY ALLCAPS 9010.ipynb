{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5486b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.1\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import AdamW\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import csv\n",
    "import re\n",
    "import validators\n",
    "import emoji\n",
    "import unidecode\n",
    "import nltk\n",
    "import pickle\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6954a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed for reproducibility\n",
    "SEED = 1235\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# BERT Hyperparameters (ADDITION)\n",
    "n_bert_layers = 16  # Assuming the base model has 12 layers\n",
    "bert_lr = 0.001\n",
    "pooling_strategy = 'cls'  # Options: 'cls', 'mean', 'max'\n",
    "bert_hidden_size = 768  # Adjust based on your BERT model\n",
    "max_seq_length = 128\n",
    "fine_tune_strategy = 'full'  # Options: 'full', 'last_layer'\n",
    "bert_dropout = 0.9  # Adjust based on BERT model specifications\n",
    "\n",
    "max_seq_length = 128  # This should match the max_seq_length used in BERT model\n",
    "padding_strategy = 'max_length'  # Options: 'max_length', 'do_not_pad', 'longest'\n",
    "truncation_strategy = 'longest_first'  # Options: 'longest_first', 'only_first', 'only_second'\n",
    "do_lower_case = True  # Set to False if using a cased model\n",
    "\n",
    "config = BertConfig(\n",
    "    num_hidden_layers=n_bert_layers,\n",
    "    hidden_size=bert_hidden_size,\n",
    "    num_attention_heads=24,  # Assuming 12 attention heads\n",
    "    intermediate_size=4 * bert_hidden_size,  # Default value in BERT\n",
    "    hidden_dropout_prob=bert_dropout,\n",
    "    attention_probs_dropout_prob=bert_dropout,\n",
    ")\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          max_length=max_seq_length,\n",
    "                                          padding=padding_strategy,\n",
    "                                          truncation=truncation_strategy,\n",
    "                                          do_lower_case=do_lower_case)\n",
    "# Load the BERT model with the custom configuration\n",
    "bert_model = BertModel(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0815b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"C:/Users/admin/Downloads/Binary_dataset.csv\"\n",
    "data_df = pd.read_csv(data_path)\n",
    "data_df = data_df.rename(columns={'Tweet Content': 'text', 'Label': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fd15764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Worst Bong ever. https://t.co/QA7R8VYppC</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what i dont like about leni robredo's platform...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ito ang tunay na survey ni VP Leni Robredo #1 ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(3) BBM sued for Pork Barrel Scam\\n\\nhttps://t...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sabog din sumagot tong si Norberto Gonzales no...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text Sentiment label\n",
       "0           Worst Bong ever. https://t.co/QA7R8VYppC  Negative  Hate\n",
       "1  what i dont like about leni robredo's platform...  Negative  Hate\n",
       "2  Ito ang tunay na survey ni VP Leni Robredo #1 ...  Negative  Hate\n",
       "3  (3) BBM sued for Pork Barrel Scam\\n\\nhttps://t...  Negative  Hate\n",
       "4  Sabog din sumagot tong si Norberto Gonzales no...  Negative  Hate"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d3add86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>I took The Blind Test and my top candidates ar...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>\"True leader show up and man up.\" - VP Leni Ro...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>Leni Robredo for president cutie ðŸ¤žðŸŒ¸</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>Ako si Christian Tan, kabataan at kaisa ni Bon...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>Ate @xlykable Letâ€™s support VP Leni and Sen. K...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3835</th>\n",
       "      <td>Just because Aiai did not supported Leni Robre...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3836</th>\n",
       "      <td>â€œMga kababayan, summon the warrior in you and ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3837</th>\n",
       "      <td>@thekiarasworld Now I know that not all of the...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3838</th>\n",
       "      <td>Ping Lacson Ang may Plano sa bansa\\n\\n#KayPing...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3839</th>\n",
       "      <td>@dfenderwoborder Pls watch &amp;amp; share.  Ang g...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text Sentiment     label\n",
       "2560  I took The Blind Test and my top candidates ar...  Positive  Non-hate\n",
       "2561  \"True leader show up and man up.\" - VP Leni Ro...  Positive  Non-hate\n",
       "2562                Leni Robredo for president cutie ðŸ¤žðŸŒ¸  Positive  Non-hate\n",
       "2563  Ako si Christian Tan, kabataan at kaisa ni Bon...  Positive  Non-hate\n",
       "2564  Ate @xlykable Letâ€™s support VP Leni and Sen. K...  Positive  Non-hate\n",
       "...                                                 ...       ...       ...\n",
       "3835  Just because Aiai did not supported Leni Robre...  Positive  Non-hate\n",
       "3836  â€œMga kababayan, summon the warrior in you and ...  Positive  Non-hate\n",
       "3837  @thekiarasworld Now I know that not all of the...  Positive  Non-hate\n",
       "3838  Ping Lacson Ang may Plano sa bansa\\n\\n#KayPing...  Positive  Non-hate\n",
       "3839  @dfenderwoborder Pls watch &amp; share.  Ang g...  Positive  Non-hate\n",
       "\n",
       "[1280 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupedby_sentiment = data_df.groupby(data_df.Sentiment)\n",
    "data_df_positive = groupedby_sentiment.get_group(\"Positive\")\n",
    "data_df_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eaa751cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Worst Bong ever. https://t.co/QA7R8VYppC</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what i dont like about leni robredo's platform...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ito ang tunay na survey ni VP Leni Robredo #1 ...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(3) BBM sued for Pork Barrel Scam\\n\\nhttps://t...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sabog din sumagot tong si Norberto Gonzales no...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2555</th>\n",
       "      <td>Headline: The ambitious presidential candidate...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2556</th>\n",
       "      <td>Norberto Gonzales is right, its a missed oppor...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2557</th>\n",
       "      <td>The audacity to call Leni Robredo \"bobo\", \" ta...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>Bongbong Marcos is a Nazi. https://t.co/gY3xHb...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>Ang humihingi ng Respeto dapat marunong din Ru...</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2560 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text Sentiment label\n",
       "0              Worst Bong ever. https://t.co/QA7R8VYppC  Negative  Hate\n",
       "1     what i dont like about leni robredo's platform...  Negative  Hate\n",
       "2     Ito ang tunay na survey ni VP Leni Robredo #1 ...  Negative  Hate\n",
       "3     (3) BBM sued for Pork Barrel Scam\\n\\nhttps://t...  Negative  Hate\n",
       "4     Sabog din sumagot tong si Norberto Gonzales no...  Negative  Hate\n",
       "...                                                 ...       ...   ...\n",
       "2555  Headline: The ambitious presidential candidate...  Negative  Hate\n",
       "2556  Norberto Gonzales is right, its a missed oppor...  Negative  Hate\n",
       "2557  The audacity to call Leni Robredo \"bobo\", \" ta...  Negative  Hate\n",
       "2558  Bongbong Marcos is a Nazi. https://t.co/gY3xHb...  Negative  Hate\n",
       "2559  Ang humihingi ng Respeto dapat marunong din Ru...  Negative  Hate\n",
       "\n",
       "[2560 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_negative = groupedby_sentiment.get_group(\"Negative\")\n",
    "data_df_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d472736d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3840</th>\n",
       "      <td>bongbong marcos dot com</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3841</th>\n",
       "      <td>Grabe pala talaga yung actions ni Leni Robredo...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3842</th>\n",
       "      <td>â€œNgayong darating na halalan, ang tatanglaw sa...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3843</th>\n",
       "      <td>For this COMELEC debate:\\n\\nValedictorian: Len...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3844</th>\n",
       "      <td>Focus on the ball kakampinks\\n\\nPresident Leni...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>President Leni Robredo and Vice President Kiko...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>@jillrobredo ðŸŒºðŸŒºðŸŒº\\nthank you din kay @maraceped...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>LOOK: Presidential candidate Bongbong Marcos m...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>@itsmaxandcheese Leni Robredo for President 2022</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>LENI ROBREDO FOR PRESIDENT. https://t.co/b14K2...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1280 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text Sentiment     label\n",
       "3840                            bongbong marcos dot com   Neutral  Non-hate\n",
       "3841  Grabe pala talaga yung actions ni Leni Robredo...   Neutral  Non-hate\n",
       "3842  â€œNgayong darating na halalan, ang tatanglaw sa...   Neutral  Non-hate\n",
       "3843  For this COMELEC debate:\\n\\nValedictorian: Len...   Neutral  Non-hate\n",
       "3844  Focus on the ball kakampinks\\n\\nPresident Leni...   Neutral  Non-hate\n",
       "...                                                 ...       ...       ...\n",
       "5115  President Leni Robredo and Vice President Kiko...   Neutral  Non-hate\n",
       "5116  @jillrobredo ðŸŒºðŸŒºðŸŒº\\nthank you din kay @maraceped...   Neutral  Non-hate\n",
       "5117  LOOK: Presidential candidate Bongbong Marcos m...   Neutral  Non-hate\n",
       "5118   @itsmaxandcheese Leni Robredo for President 2022   Neutral  Non-hate\n",
       "5119  LENI ROBREDO FOR PRESIDENT. https://t.co/b14K2...   Neutral  Non-hate\n",
       "\n",
       "[1280 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df_neutral = groupedby_sentiment.get_group(\"Neutral\")\n",
    "data_df_neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9d20c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_23620\\2699631638.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df_nonhate = data_df_positive.append(data_df_neutral)\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_23620\\2699631638.py:9: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data_df = data_df_hate.append(data_df_nonhate)\n"
     ]
    }
   ],
   "source": [
    "#binary hate non-hate\n",
    "data_df_hate = data_df_negative.sample(n = 2560, replace=True)\n",
    "\n",
    "data_df_positive = data_df_positive.sample(n = 1280, replace=True)\n",
    "data_df_neutral = data_df_neutral.sample(n = 1280, replace=True)\n",
    "\n",
    "data_df_nonhate = data_df_positive.append(data_df_neutral)\n",
    "\n",
    "data_df = data_df_hate.append(data_df_nonhate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56cefa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.drop(['Sentiment'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6f1a120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>Winners and Losers of the CNN Philippines Pres...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>@josef_ethanol MarcosJr. duwag. Ayaw umattend ...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>@freddieamor03 @Jericho30098056 @delia_montero...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1728</th>\n",
       "      <td>Satanas and President Leni Robredo are both tr...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>si bongbong marcos ay tamad. duwag. tax evader...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3906</th>\n",
       "      <td>*Disclaimer: neither @onegai_ph nor the people...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>ATM: Inday Sara Duterte, emosyonal sa miting d...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4908</th>\n",
       "      <td>â€œTrue leaders show up and man up. Kaya po sa d...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4421</th>\n",
       "      <td>yung kasalanan ni Ferdinand Marcos wag daw isi...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4809</th>\n",
       "      <td>LENI ROBREDO #10 SA BALOTA\\nKIKO PANGILINAN #7...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5120 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text     label\n",
       "628   Winners and Losers of the CNN Philippines Pres...      Hate\n",
       "2105  @josef_ethanol MarcosJr. duwag. Ayaw umattend ...      Hate\n",
       "529   @freddieamor03 @Jericho30098056 @delia_montero...      Hate\n",
       "1728  Satanas and President Leni Robredo are both tr...      Hate\n",
       "1173  si bongbong marcos ay tamad. duwag. tax evader...      Hate\n",
       "...                                                 ...       ...\n",
       "3906  *Disclaimer: neither @onegai_ph nor the people...  Non-hate\n",
       "5057  ATM: Inday Sara Duterte, emosyonal sa miting d...  Non-hate\n",
       "4908  â€œTrue leaders show up and man up. Kaya po sa d...  Non-hate\n",
       "4421  yung kasalanan ni Ferdinand Marcos wag daw isi...  Non-hate\n",
       "4809  LENI ROBREDO #10 SA BALOTA\\nKIKO PANGILINAN #7...  Non-hate\n",
       "\n",
       "[5120 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_df.to_csv('dataset.csv', index=False)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "803cceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f10c2e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628</td>\n",
       "      <td>Winners and Losers of the CNN Philippines Pres...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2105</td>\n",
       "      <td>@josef_ethanol MarcosJr. duwag. Ayaw umattend ...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>529</td>\n",
       "      <td>@freddieamor03 @Jericho30098056 @delia_montero...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1728</td>\n",
       "      <td>Satanas and President Leni Robredo are both tr...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1173</td>\n",
       "      <td>si bongbong marcos ay tamad. duwag. tax evader...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>3906</td>\n",
       "      <td>*Disclaimer: neither @onegai_ph nor the people...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>5057</td>\n",
       "      <td>ATM: Inday Sara Duterte, emosyonal sa miting d...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>4908</td>\n",
       "      <td>â€œTrue leaders show up and man up. Kaya po sa d...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>4421</td>\n",
       "      <td>yung kasalanan ni Ferdinand Marcos wag daw isi...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>4809</td>\n",
       "      <td>LENI ROBREDO #10 SA BALOTA\\nKIKO PANGILINAN #7...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5120 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text     label\n",
       "0       628  Winners and Losers of the CNN Philippines Pres...      Hate\n",
       "1      2105  @josef_ethanol MarcosJr. duwag. Ayaw umattend ...      Hate\n",
       "2       529  @freddieamor03 @Jericho30098056 @delia_montero...      Hate\n",
       "3      1728  Satanas and President Leni Robredo are both tr...      Hate\n",
       "4      1173  si bongbong marcos ay tamad. duwag. tax evader...      Hate\n",
       "...     ...                                                ...       ...\n",
       "5115   3906  *Disclaimer: neither @onegai_ph nor the people...  Non-hate\n",
       "5116   5057  ATM: Inday Sara Duterte, emosyonal sa miting d...  Non-hate\n",
       "5117   4908  â€œTrue leaders show up and man up. Kaya po sa d...  Non-hate\n",
       "5118   4421  yung kasalanan ni Ferdinand Marcos wag daw isi...  Non-hate\n",
       "5119   4809  LENI ROBREDO #10 SA BALOTA\\nKIKO PANGILINAN #7...  Non-hate\n",
       "\n",
       "[5120 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c63582fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b8afaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = tokens[:tokenizer.model_max_length - 2]  # Account for [CLS] and [SEP] tokens\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return indexed_tokens\n",
    "\n",
    "filipino_stopwords = set(\n",
    "    \"\"\"\n",
    "akin\n",
    "aking\n",
    "ako\n",
    "alin\n",
    "am\n",
    "amin\n",
    "aming\n",
    "ang\n",
    "ano\n",
    "anumang\n",
    "apat\n",
    "at\n",
    "atin\n",
    "ating\n",
    "ay\n",
    "bababa\n",
    "bago\n",
    "bakit\n",
    "bawat\n",
    "bilang\n",
    "dahil\n",
    "dalawa\n",
    "dapat\n",
    "din\n",
    "dito\n",
    "doon\n",
    "gagawin\n",
    "gayunman\n",
    "ginagawa\n",
    "ginawa\n",
    "ginawang\n",
    "gumawa\n",
    "gusto\n",
    "habang\n",
    "hanggang\n",
    "hindi\n",
    "huwag\n",
    "iba\n",
    "ibaba\n",
    "ibabaw\n",
    "ibig\n",
    "ikaw\n",
    "ilagay\n",
    "ilalim\n",
    "ilan\n",
    "inyong\n",
    "isa\n",
    "isang\n",
    "itaas\n",
    "ito\n",
    "iyo\n",
    "iyon\n",
    "iyong\n",
    "ka\n",
    "kahit\n",
    "kailangan\n",
    "kailanman\n",
    "kami\n",
    "kanila\n",
    "kanilang\n",
    "kanino\n",
    "kanya\n",
    "kanyang\n",
    "kapag\n",
    "kapwa\n",
    "karamihan\n",
    "katiyakan\n",
    "katulad\n",
    "kaya\n",
    "kaysa\n",
    "ko\n",
    "kong\n",
    "kulang\n",
    "kumuha\n",
    "kung\n",
    "laban\n",
    "lahat\n",
    "lamang\n",
    "likod\n",
    "lima\n",
    "maaari\n",
    "maaaring\n",
    "maging\n",
    "mahusay\n",
    "makita\n",
    "marami\n",
    "marapat\n",
    "masyado\n",
    "may\n",
    "mayroon\n",
    "mga\n",
    "minsan\n",
    "mismo\n",
    "mula\n",
    "muli\n",
    "na\n",
    "nabanggit\n",
    "naging\n",
    "nagkaroon\n",
    "nais\n",
    "nakita\n",
    "namin\n",
    "napaka\n",
    "narito\n",
    "nasaan\n",
    "ng\n",
    "ngayon\n",
    "ni\n",
    "nila\n",
    "nilang\n",
    "nito\n",
    "niya\n",
    "niyang\n",
    "noon\n",
    "o\n",
    "pa\n",
    "paano\n",
    "pababa\n",
    "paggawa\n",
    "pagitan\n",
    "pagkakaroon\n",
    "pagkatapos\n",
    "palabas\n",
    "pamamagitan\n",
    "panahon\n",
    "pangalawa\n",
    "para\n",
    "paraan\n",
    "pareho\n",
    "pataas\n",
    "pero\n",
    "pumunta\n",
    "pumupunta\n",
    "sa\n",
    "saan\n",
    "sabi\n",
    "sabihin\n",
    "sarili\n",
    "sila\n",
    "sino\n",
    "siya\n",
    "tatlo\n",
    "tayo\n",
    "tulad\n",
    "tungkol\n",
    "una\n",
    "walang\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "# Date De-Identification\n",
    "def remove_mentions(text):\n",
    "    mention_pattern = re.compile(r'@\\w+')\n",
    "    \n",
    "    # Use re.sub to remove mentions\n",
    "    cleaned_text = mention_pattern.sub('', text)\n",
    "    \n",
    "    # Remove extra spaces and strip leading/trailing spaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# URL Removal\n",
    "def remove_url(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    \n",
    "    # Use re.sub to remove URLs\n",
    "    cleaned_text = url_pattern.sub('', text)\n",
    "    \n",
    "    # Remove extra spaces and strip leading/trailing spaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Special Characters Removal\n",
    "def remove_special_characters(text):\n",
    "    text = emoji.replace_emoji(text, replace=\"[emoji]\")\n",
    "    \n",
    "    # Split the text into words\n",
    "    words = text.split(\" \")\n",
    "    \n",
    "    # Initialize an empty string to store the cleaned text\n",
    "    cleaned_text = \"\"\n",
    "    \n",
    "    # Iterate through each word\n",
    "    for word in words:\n",
    "        # Check if the word contains only special characters or \"[emoji]\"\n",
    "        if not (re.match(r\"^[_\\W]+$\", word) or \"[emoji]\" in word):\n",
    "            if len(cleaned_text) == 0:\n",
    "                cleaned_text = f\"{word}\"\n",
    "            else:\n",
    "                cleaned_text = f\"{cleaned_text} {word}\"\n",
    "                \n",
    "    # Remove diacritics\n",
    "    text_no_diacritics = unidecode.unidecode(cleaned_text)\n",
    "\n",
    "    # Split the text into words\n",
    "    sentence = text_no_diacritics.split(\" \")\n",
    "    output = \"\"\n",
    "\n",
    "    # Remove special characters and numerics\n",
    "    for part in sentence:\n",
    "        part = re.sub(\"[^A-Za-z ]+$\", \"\", part)\n",
    "        part = re.sub(\"^[^A-Za-z #]+\", \"\", part)\n",
    "        if not (len(part) <= 1 or re.match(r\"[^a-zA-Z]\", part)):\n",
    "            if len(output) == 0:\n",
    "                output = f\"{part}\"\n",
    "            else:\n",
    "                output = f\"{output} {part}\"\n",
    "\n",
    "    # Remove extra spaces and strip leading/trailing spaces\n",
    "    cleaned_text = ' '.join(output.split())\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "# Remove English Stop Words\n",
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "# Remove English Stop Words\n",
    "def remove_english_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    \n",
    "    # Remove extra spaces and strip leading/trailing spaces\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Remove Filipino Stop Words\n",
    "def remove_filipino_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in filipino_stopwords]\n",
    "    \n",
    "    # Remove extra spaces and strip leading/trailing spaces\n",
    "    cleaned_text = ' '.join(filtered_words)\n",
    "  \n",
    "    return cleaned_text\n",
    "\n",
    "# Candidate Name Removal\n",
    "def remove_candidate_names(text):\n",
    "    candidatelist = \"leni robredo bongbong marcos isko moreno domagoso manny pacman pacquiao ping lacson ernie abella leody de guzman norberto gonzales jose montemayor jr faisal mangondato\"\n",
    "    candidatelist = candidatelist.split()\n",
    "    candidate_pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, candidatelist)) + r')\\b', re.IGNORECASE)\n",
    "    \n",
    "    # Use re.sub to remove candidate names\n",
    "    cleaned_text = candidate_pattern.sub('', text)\n",
    "    \n",
    "    # Remove extra spaces and strip leading/trailing spaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "        \n",
    "    return cleaned_text\n",
    "\n",
    "# Hashtag Removal\n",
    "def remove_hashtags(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Initialize an empty list to store cleaned words\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        # Check if the word is a hashtag (starts with #)\n",
    "        if not word.startswith('#'):\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    # Join the cleaned words into a single string\n",
    "    cleaned_text = ' '.join(cleaned_words)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Hashtag Removal\n",
    "def remove_hashtag_symbols(text):\n",
    "    # Use regular expression to remove \"#\" before words\n",
    "    cleaned_text = re.sub(r'#(\\w+)', r'\\1', text)\n",
    "    \n",
    "    # Remove extra spaces and strip leading/trailing spaces\n",
    "    cleaned_text = ' '.join(cleaned_text.split())\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af5cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_except_all_caps(text):\n",
    "    cleaned_text = remove_hashtag_symbols(text)\n",
    "    \n",
    "    words = cleaned_text.split()\n",
    "    filtered_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.isupper() and not word.istitle():\n",
    "            filtered_words.append(word)\n",
    "        else:\n",
    "            filtered_words.append(re.sub(r'([A-Z][a-z]+)', lambda x: x.group(1).lower(), word))\n",
    "            \n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b3226a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad02bb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Winners and Losers of the CNN Philippines Presidential Debate: Winners: VP Leni Robredo and Ka Leody De Guzman Neutral: Manny Pacquiao, Ernesto Abella, Ping Lacson Losers: Jose Montemayor Isko Moreno Norberto Gonzales Faisal Mangondato Biggest Loser: Bongbong Marcos \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  MarcosJr. duwag. Ayaw umattend ng mga debate dahil hindi kayang ipagtanggol ang sarili at pamilya nya. Kung ang sarili at pamilya nya nga ay hindi nya kayang ipagtanggol at panindigan, ang sambayanang Pilipino pa kaya? Gising Pilipinas!!! LENI ROBREDO FOR THE WIN!!! ðŸ™ðŸ’—ðŸ’—ðŸ’—ðŸ™ \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  , pls investigate this troll for spreading malicious disinformation about Leni Robredo. This thread is replete of evidence that the troll committed cyber crime and libel. \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  Satanas and President Leni Robredo are both trending on Twitter today. Does this mean Satanas si Leni Robredo??? Just asking... \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  si bongbong marcos ay tamad. duwag. tax evader. sinungaling. at higit sa lahat - WALANG PAKIALAM SA IYO. https://t.co/4XGRM181PP \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  #ashsgs BONGBONG MARCOS . COM WHAHSHSKAMMEMAKAKA YOUTUBE O \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  â€œDealing with the public is a choreâ€ - Bongbong Marcos Diring diri yarn? #BBMSaraUNITEAM ðŸ¤®ðŸ’©ðŸ¤¡ #BringBackMarcos ðŸ’©ðŸ¤¡ðŸ¤® #PilipinasIsPink #Ipana7oNa10ParaSaLahat https://t.co/J5N4xEj2uJ \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  Mental institute of the Phils. Vs Leni lutang Robredo. Make senseðŸ˜‚ \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  ANG TAO KAPAG KAYA MAG SINUNGALING SA MALIIT NA BAGAY KAYA RIN MAG SINUNGALING PAG DATING SA MALAKING BAGAY. OUR FUTURE President Leni Robredo ðŸŒ¸ðŸŒ·ðŸ’®ðŸª· #NagaIsPink #SorsogonIsPink #BicolIsPink https://t.co/QgR1uWjjt1 \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  I donâ€™t see any evidence Bongbong Marcos stole from the public in this â€œeducate yourselfâ€ response. By the way, the burden lies on the accuser to provide evidence. You canâ€™t tell me to do my own research! https://t.co/KWRcmJGqCx \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data De-Identification\n",
    "data_df['text'] = data_df['text'].apply(remove_mentions)\n",
    "\n",
    "for i in range(10):\n",
    "    text = data_df[\"text\"][i]\n",
    "    label = data_df[\"label\"][i]\n",
    "\n",
    "    print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1899b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Winners and Losers of the CNN Philippines Presidential Debate: Winners: VP Leni Robredo and Ka Leody De Guzman Neutral: Manny Pacquiao, Ernesto Abella, Ping Lacson Losers: Jose Montemayor Isko Moreno Norberto Gonzales Faisal Mangondato Biggest Loser: Bongbong Marcos \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  MarcosJr. duwag. Ayaw umattend ng mga debate dahil hindi kayang ipagtanggol ang sarili at pamilya nya. Kung ang sarili at pamilya nya nga ay hindi nya kayang ipagtanggol at panindigan, ang sambayanang Pilipino pa kaya? Gising Pilipinas!!! LENI ROBREDO FOR THE WIN!!! ðŸ™ðŸ’—ðŸ’—ðŸ’—ðŸ™ \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  , pls investigate this troll for spreading malicious disinformation about Leni Robredo. This thread is replete of evidence that the troll committed cyber crime and libel. \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  Satanas and President Leni Robredo are both trending on Twitter today. Does this mean Satanas si Leni Robredo??? Just asking... \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  si bongbong marcos ay tamad. duwag. tax evader. sinungaling. at higit sa lahat - WALANG PAKIALAM SA IYO. \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  #ashsgs BONGBONG MARCOS . COM WHAHSHSKAMMEMAKAKA YOUTUBE O \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  â€œDealing with the public is a choreâ€ - Bongbong Marcos Diring diri yarn? #BBMSaraUNITEAM ðŸ¤®ðŸ’©ðŸ¤¡ #BringBackMarcos ðŸ’©ðŸ¤¡ðŸ¤® #PilipinasIsPink #Ipana7oNa10ParaSaLahat \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  Mental institute of the Phils. Vs Leni lutang Robredo. Make senseðŸ˜‚ \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  ANG TAO KAPAG KAYA MAG SINUNGALING SA MALIIT NA BAGAY KAYA RIN MAG SINUNGALING PAG DATING SA MALAKING BAGAY. OUR FUTURE President Leni Robredo ðŸŒ¸ðŸŒ·ðŸ’®ðŸª· #NagaIsPink #SorsogonIsPink #BicolIsPink \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  I donâ€™t see any evidence Bongbong Marcos stole from the public in this â€œeducate yourselfâ€ response. By the way, the burden lies on the accuser to provide evidence. You canâ€™t tell me to do my own research! \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# URL Removal\n",
    "data_df['text'] = data_df['text'].apply(remove_url)\n",
    "\n",
    "for i in range(10):\n",
    "    text = data_df[\"text\"][i]\n",
    "    label = data_df[\"label\"][i]\n",
    "\n",
    "    print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50e8dc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Winners and Losers of the CNN Philippines Presidential Debate Winners VP Leni Robredo and Ka Leody De Guzman Neutral Manny Pacquiao Ernesto Abella Ping Lacson Losers Jose Montemayor Isko Moreno Norberto Gonzales Faisal Mangondato Biggest Loser Bongbong Marcos \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  MarcosJr duwag Ayaw umattend ng mga debate dahil hindi kayang ipagtanggol ang sarili at pamilya nya Kung ang sarili at pamilya nya nga ay hindi nya kayang ipagtanggol at panindigan ang sambayanang Pilipino pa kaya Gising Pilipinas LENI ROBREDO FOR THE WIN \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  pls investigate this troll for spreading malicious disinformation about Leni Robredo This thread is replete of evidence that the troll committed cyber crime and libel \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  Satanas and President Leni Robredo are both trending on Twitter today Does this mean Satanas si Leni Robredo Just asking \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  si bongbong marcos ay tamad duwag tax evader sinungaling at higit sa lahat WALANG PAKIALAM SA IYO \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  BONGBONG MARCOS COM WHAHSHSKAMMEMAKAKA YOUTUBE \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  Dealing with the public is chore Bongbong Marcos Diring diri yarn \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  Mental institute of the Phils Vs Leni lutang Robredo Make \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  ANG TAO KAPAG KAYA MAG SINUNGALING SA MALIIT NA BAGAY KAYA RIN MAG SINUNGALING PAG DATING SA MALAKING BAGAY OUR FUTURE President Leni Robredo \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  don't see any evidence Bongbong Marcos stole from the public in this educate yourself response By the way the burden lies on the accuser to provide evidence You can't tell me to do my own research \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Special Characters Removal\n",
    "data_df['text'] = data_df['text'].apply(remove_special_characters)\n",
    "\n",
    "for i in range(10):\n",
    "    text = data_df[\"text\"][i]\n",
    "    label = data_df[\"label\"][i]\n",
    "\n",
    "    print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b2db5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  winners and losers of the CNN philippines presidential debate winners VP leni robredo and ka leody de guzman neutral manny pacquiao ernesto abella ping lacson losers jose montemayor isko moreno norberto gonzales faisal mangondato biggest loser bongbong marcos \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  marcosjr duwag ayaw umattend ng mga debate dahil hindi kayang ipagtanggol ang sarili at pamilya nya kung ang sarili at pamilya nya nga ay hindi nya kayang ipagtanggol at panindigan ang sambayanang pilipino pa kaya gising pilipinas LENI ROBREDO FOR THE WIN \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  pls investigate this troll for spreading malicious disinformation about leni robredo this thread is replete of evidence that the troll committed cyber crime and libel \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  satanas and president leni robredo are both trending on twitter today does this mean satanas si leni robredo just asking \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  si bongbong marcos ay tamad duwag tax evader sinungaling at higit sa lahat WALANG PAKIALAM SA IYO \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  BONGBONG MARCOS COM WHAHSHSKAMMEMAKAKA YOUTUBE \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  dealing with the public is chore bongbong marcos diring diri yarn \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  mental institute of the phils vs leni lutang robredo make \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  ANG TAO KAPAG KAYA MAG SINUNGALING SA MALIIT NA BAGAY KAYA RIN MAG SINUNGALING PAG DATING SA MALAKING BAGAY OUR FUTURE president leni robredo \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  don't see any evidence bongbong marcos stole from the public in this educate yourself response by the way the burden lies on the accuser to provide evidence you can't tell me to do my own research \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lowercase\n",
    "data_df['text'] = data_df['text'].apply(lowercase_except_all_caps)\n",
    "\n",
    "for i in range(10):\n",
    "    text = data_df[\"text\"][i]\n",
    "    label = data_df[\"label\"][i]\n",
    "\n",
    "    print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb92a4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove English Stop Words\n",
    "# data_df['text'] = data_df['text'].apply(remove_english_stopwords)\n",
    "\n",
    "# for i in range(10):\n",
    "#    text = data_df[\"text\"][i]\n",
    "#    label = data_df[\"label\"][i]\n",
    "\n",
    "#    print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86e9cf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Filipino Stop Words\n",
    "# data_df['text'] = data_df['text'].apply(remove_filipino_stopwords)\n",
    "\n",
    "# for i in range(10):\n",
    "#    text = data_df[\"text\"][i]\n",
    "#    label = data_df[\"label\"][i]\n",
    "\n",
    "#    print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f586bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Candidate Names\n",
    "# data_df['text'] = data_df['text'].apply(remove_candidate_names)\n",
    "\n",
    "# for i in range(10):\n",
    "    # text = data_df[\"text\"][i]\n",
    "    # label = data_df[\"label\"][i]\n",
    "\n",
    "    # print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2e98e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  winners and losers of the CNN philippines presidential debate winners VP leni robredo and ka leody de guzman neutral manny pacquiao ernesto abella ping lacson losers jose montemayor isko moreno norberto gonzales faisal mangondato biggest loser bongbong marcos \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  marcosjr duwag ayaw umattend ng mga debate dahil hindi kayang ipagtanggol ang sarili at pamilya nya kung ang sarili at pamilya nya nga ay hindi nya kayang ipagtanggol at panindigan ang sambayanang pilipino pa kaya gising pilipinas LENI ROBREDO FOR THE WIN \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  pls investigate this troll for spreading malicious disinformation about leni robredo this thread is replete of evidence that the troll committed cyber crime and libel \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  satanas and president leni robredo are both trending on twitter today does this mean satanas si leni robredo just asking \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  si bongbong marcos ay tamad duwag tax evader sinungaling at higit sa lahat WALANG PAKIALAM SA IYO \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  BONGBONG MARCOS COM WHAHSHSKAMMEMAKAKA YOUTUBE \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  dealing with the public is chore bongbong marcos diring diri yarn \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  mental institute of the phils vs leni lutang robredo make \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  ANG TAO KAPAG KAYA MAG SINUNGALING SA MALIIT NA BAGAY KAYA RIN MAG SINUNGALING PAG DATING SA MALAKING BAGAY OUR FUTURE president leni robredo \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  don't see any evidence bongbong marcos stole from the public in this educate yourself response by the way the burden lies on the accuser to provide evidence you can't tell me to do my own research \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Hashtags\n",
    "data_df['text'] = data_df['text'].apply(remove_hashtags)\n",
    "    \n",
    "for i in range(10):\n",
    "    text = data_df[\"text\"][i]\n",
    "    label = data_df[\"label\"][i]\n",
    "    \n",
    "    print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7eb4c5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  winners and losers of the CNN philippines presidential debate winners VP leni robredo and ka leody de guzman neutral manny pacquiao ernesto abella ping lacson losers jose montemayor isko moreno norberto gonzales faisal mangondato biggest loser bongbong marcos \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  marcosjr duwag ayaw umattend ng mga debate dahil hindi kayang ipagtanggol ang sarili at pamilya nya kung ang sarili at pamilya nya nga ay hindi nya kayang ipagtanggol at panindigan ang sambayanang pilipino pa kaya gising pilipinas LENI ROBREDO FOR THE WIN \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  pls investigate this troll for spreading malicious disinformation about leni robredo this thread is replete of evidence that the troll committed cyber crime and libel \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  satanas and president leni robredo are both trending on twitter today does this mean satanas si leni robredo just asking \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  si bongbong marcos ay tamad duwag tax evader sinungaling at higit sa lahat WALANG PAKIALAM SA IYO \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  BONGBONG MARCOS COM WHAHSHSKAMMEMAKAKA YOUTUBE \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  dealing with the public is chore bongbong marcos diring diri yarn \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  mental institute of the phils vs leni lutang robredo make \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  ANG TAO KAPAG KAYA MAG SINUNGALING SA MALIIT NA BAGAY KAYA RIN MAG SINUNGALING PAG DATING SA MALAKING BAGAY OUR FUTURE president leni robredo \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n",
      "Text:  don't see any evidence bongbong marcos stole from the public in this educate yourself response by the way the burden lies on the accuser to provide evidence you can't tell me to do my own research \n",
      "\n",
      "Label:  Hate \n",
      "\n",
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove Hashtag Symbols\n",
    "data_df['text'] = data_df['text'].apply(remove_hashtag_symbols)\n",
    "\n",
    "for i in range(10):\n",
    "    text = data_df[\"text\"][i]\n",
    "    label = data_df[\"label\"][i]\n",
    "\n",
    "    print('Text: ', text, \"\\n\\nLabel: \", label, \"\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63b091cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.to_csv('binary12.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7cc88bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628</td>\n",
       "      <td>winners and losers of the CNN philippines pres...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2105</td>\n",
       "      <td>marcosjr duwag ayaw umattend ng mga debate dah...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>529</td>\n",
       "      <td>pls investigate this troll for spreading malic...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1728</td>\n",
       "      <td>satanas and president leni robredo are both tr...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1173</td>\n",
       "      <td>si bongbong marcos ay tamad duwag tax evader s...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>3906</td>\n",
       "      <td>disclaimer neither nor the people behind this ...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>5057</td>\n",
       "      <td>ATM inday sara duterte emosyonal sa miting de ...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>4908</td>\n",
       "      <td>true leaders show up and man up kaya po sa dar...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>4421</td>\n",
       "      <td>yung kasalanan ni ferdinand marcos wag daw isi...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>4809</td>\n",
       "      <td>LENI ROBREDO SA BALOTA KIKO PANGILINAN SA BALO...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5120 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text     label\n",
       "0       628  winners and losers of the CNN philippines pres...      Hate\n",
       "1      2105  marcosjr duwag ayaw umattend ng mga debate dah...      Hate\n",
       "2       529  pls investigate this troll for spreading malic...      Hate\n",
       "3      1728  satanas and president leni robredo are both tr...      Hate\n",
       "4      1173  si bongbong marcos ay tamad duwag tax evader s...      Hate\n",
       "...     ...                                                ...       ...\n",
       "5115   3906  disclaimer neither nor the people behind this ...  Non-hate\n",
       "5116   5057  ATM inday sara duterte emosyonal sa miting de ...  Non-hate\n",
       "5117   4908  true leaders show up and man up kaya po sa dar...  Non-hate\n",
       "5118   4421  yung kasalanan ni ferdinand marcos wag daw isi...  Non-hate\n",
       "5119   4809  LENI ROBREDO SA BALOTA KIKO PANGILINAN SA BALO...  Non-hate\n",
       "\n",
       "[5120 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "364e034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['text'] = data_df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f32559d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628</td>\n",
       "      <td>[4791, 1998, 23160, 1997, 1996, 13229, 5137, 4...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2105</td>\n",
       "      <td>[14810, 3501, 2099, 4241, 4213, 2290, 1037, 31...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>529</td>\n",
       "      <td>[20228, 2015, 8556, 2023, 18792, 2005, 9359, 2...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1728</td>\n",
       "      <td>[16795, 3022, 1998, 2343, 18798, 2072, 6487, 2...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1173</td>\n",
       "      <td>[9033, 14753, 18259, 5063, 14810, 1037, 2100, ...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>3906</td>\n",
       "      <td>[5860, 19771, 5017, 4445, 4496, 1996, 2111, 23...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>5057</td>\n",
       "      <td>[27218, 27427, 4710, 7354, 4241, 3334, 2618, 7...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>4908</td>\n",
       "      <td>[2995, 4177, 2265, 2039, 1998, 2158, 2039, 109...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>4421</td>\n",
       "      <td>[22854, 2290, 10556, 12002, 5162, 2078, 9152, ...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>4809</td>\n",
       "      <td>[18798, 2072, 6487, 23417, 7842, 28352, 17287,...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5120 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text     label\n",
       "0       628  [4791, 1998, 23160, 1997, 1996, 13229, 5137, 4...      Hate\n",
       "1      2105  [14810, 3501, 2099, 4241, 4213, 2290, 1037, 31...      Hate\n",
       "2       529  [20228, 2015, 8556, 2023, 18792, 2005, 9359, 2...      Hate\n",
       "3      1728  [16795, 3022, 1998, 2343, 18798, 2072, 6487, 2...      Hate\n",
       "4      1173  [9033, 14753, 18259, 5063, 14810, 1037, 2100, ...      Hate\n",
       "...     ...                                                ...       ...\n",
       "5115   3906  [5860, 19771, 5017, 4445, 4496, 1996, 2111, 23...  Non-hate\n",
       "5116   5057  [27218, 27427, 4710, 7354, 4241, 3334, 2618, 7...  Non-hate\n",
       "5117   4908  [2995, 4177, 2265, 2039, 1998, 2158, 2039, 109...  Non-hate\n",
       "5118   4421  [22854, 2290, 10556, 12002, 5162, 2078, 9152, ...  Non-hate\n",
       "5119   4809  [18798, 2072, 6487, 23417, 7842, 28352, 17287,...  Non-hate\n",
       "\n",
       "[5120 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "274bc204",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_df, test_size=0.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3706e8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dropout = nn.Dropout(dropout) \n",
    "        self.conv1 = nn.Conv1d(in_channels=embedding_dim, out_channels=256, kernel_size=2, padding='same')\n",
    "        self.pool1 = nn.MaxPool1d(2, 2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=256, out_channels=64, kernel_size=4, padding='same')\n",
    "        self.global_pooling = nn.AdaptiveMaxPool1d(1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        x = embedded.permute(0, 2, 1)  # Change the dimensions for convolution\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.global_pooling(x).squeeze(2)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9d73f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up iterators\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a9a6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, max_seq_length):\n",
    "        self.data = dataframe\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        label = self.data.iloc[idx]['label']\n",
    "\n",
    "        # Padding and conversion to tensor\n",
    "        padded_text = torch.tensor(text[:self.max_seq_length] + [0] * (self.max_seq_length - len(text)))\n",
    "        return padded_text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2db27b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(train_df, 1000)\n",
    "test_dataset = TextDataset(test_df, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "782d32d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_iterator = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce1934a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBEDDING_DIM = 768\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3, 4, 5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# CNN Hyperparameters\n",
    "hidden_dim = 100\n",
    "n_conv_layers = 1\n",
    "kernel_sizes = [2, 3, 4]\n",
    "activation = nn.ReLU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "657020f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CNN model\n",
    "model = CNN(VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "#Initialize CNN model\n",
    "# model = CNN(VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, hidden_dim, n_conv_layers, kernel_sizes, activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6896c237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.9, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.9, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.9, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize BERT model (for embedding extraction)\n",
    "bert_model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d06a906b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, token in enumerate(tokenizer.get_vocab()):\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        token_embedding = bert_model.embeddings.word_embeddings.weight[token_id]\n",
    "        model.embedding.weight[i].data.copy_(token_embedding)\n",
    "\n",
    "bert_parameters = []\n",
    "for layer in bert_model.encoder.layer:\n",
    "    bert_parameters.extend(layer.parameters())\n",
    "\n",
    "# Create AdamW optimizer with custom hyperparameters for BERT embeddings\n",
    "bert_learning_rate = 2e-4  # Adjust as needed\n",
    "bert_optimizer = optim.AdamW(bert_parameters, lr=bert_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc3596cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>628</td>\n",
       "      <td>[4791, 1998, 23160, 1997, 1996, 13229, 5137, 4...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2105</td>\n",
       "      <td>[14810, 3501, 2099, 4241, 4213, 2290, 1037, 31...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>529</td>\n",
       "      <td>[20228, 2015, 8556, 2023, 18792, 2005, 9359, 2...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1728</td>\n",
       "      <td>[16795, 3022, 1998, 2343, 18798, 2072, 6487, 2...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1173</td>\n",
       "      <td>[9033, 14753, 18259, 5063, 14810, 1037, 2100, ...</td>\n",
       "      <td>Hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5115</th>\n",
       "      <td>3906</td>\n",
       "      <td>[5860, 19771, 5017, 4445, 4496, 1996, 2111, 23...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5116</th>\n",
       "      <td>5057</td>\n",
       "      <td>[27218, 27427, 4710, 7354, 4241, 3334, 2618, 7...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5117</th>\n",
       "      <td>4908</td>\n",
       "      <td>[2995, 4177, 2265, 2039, 1998, 2158, 2039, 109...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>4421</td>\n",
       "      <td>[22854, 2290, 10556, 12002, 5162, 2078, 9152, ...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>4809</td>\n",
       "      <td>[18798, 2072, 6487, 23417, 7842, 28352, 17287,...</td>\n",
       "      <td>Non-hate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5120 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text     label\n",
       "0       628  [4791, 1998, 23160, 1997, 1996, 13229, 5137, 4...      Hate\n",
       "1      2105  [14810, 3501, 2099, 4241, 4213, 2290, 1037, 31...      Hate\n",
       "2       529  [20228, 2015, 8556, 2023, 18792, 2005, 9359, 2...      Hate\n",
       "3      1728  [16795, 3022, 1998, 2343, 18798, 2072, 6487, 2...      Hate\n",
       "4      1173  [9033, 14753, 18259, 5063, 14810, 1037, 2100, ...      Hate\n",
       "...     ...                                                ...       ...\n",
       "5115   3906  [5860, 19771, 5017, 4445, 4496, 1996, 2111, 23...  Non-hate\n",
       "5116   5057  [27218, 27427, 4710, 7354, 4241, 3334, 2618, 7...  Non-hate\n",
       "5117   4908  [2995, 4177, 2265, 2039, 1998, 2158, 2039, 109...  Non-hate\n",
       "5118   4421  [22854, 2290, 10556, 12002, 5162, 2078, 9152, ...  Non-hate\n",
       "5119   4809  [18798, 2072, 6487, 23417, 7842, 28352, 17287,...  Non-hate\n",
       "\n",
       "[5120 rows x 3 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "02c4f8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = optim.Adam(model.parameters())\n",
    "# Your custom hyperparameters\n",
    "learning_rate = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-08\n",
    "weight_decay = 0.0\n",
    "\n",
    "# Create Adam optimizer with custom hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1, beta_2), eps=epsilon, weight_decay=weight_decay)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e33b7da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train(model, iterator):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for text_batch, label_batch in iterator:\n",
    "        # Extract text sequences from the text_batch tensor\n",
    "        texts = text_batch\n",
    "        \n",
    "        # Extract and process labels\n",
    "        labels = [1 if label == 'Hate' else 0 for label in label_batch]  # Example conversion\n",
    "        \n",
    "        texts = texts.to(device)  # Move to device if needed\n",
    "        labels = torch.tensor(labels, dtype=torch.float32).to(device)  # Convert to tensor\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(texts).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "\n",
    "# Evaluate function\n",
    "def evaluate(model, iterator):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    predicted_labels = []\n",
    "    true_labels = []  # Declare the true_labels list\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for text_batch, label_batch in iterator:\n",
    "            texts = text_batch  # Extract text sequences\n",
    "            labels = [1 if label == 'Hate' else 0 for label in label_batch]  # Example conversion\n",
    "            \n",
    "            texts = texts.to(device)  # Move to device\n",
    "            labels = torch.tensor(labels, dtype=torch.float32).to(device)  # Convert to tensor\n",
    "            \n",
    "            predictions = model(texts).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            predicted_labels.extend(torch.round(torch.sigmoid(predictions)).cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate accuracy, f1, precision, recall\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels)\n",
    "    recall = recall_score(true_labels, predicted_labels)\n",
    "    \n",
    "    return epoch_loss / len(iterator), accuracy, f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec217d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:303: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  C:\\b\\abs_bao0hdcrdh\\croot\\pytorch_1675190257512\\work\\aten\\src\\ATen\\native\\Convolution.cpp:883.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.571\n",
      "\tTest Loss: 0.393\n",
      "\tAccuracy: 0.8320 | F1-Score: 0.8307\n",
      "\tPrecision: 0.8242 | Recall: 0.8373\n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.312\n",
      "\tTest Loss: 0.299\n",
      "\tAccuracy: 0.8613 | F1-Score: 0.8524\n",
      "\tPrecision: 0.8952 | Recall: 0.8135\n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.179\n",
      "\tTest Loss: 0.263\n",
      "\tAccuracy: 0.9121 | F1-Score: 0.9098\n",
      "\tPrecision: 0.9190 | Recall: 0.9008\n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.100\n",
      "\tTest Loss: 0.294\n",
      "\tAccuracy: 0.9141 | F1-Score: 0.9127\n",
      "\tPrecision: 0.9127 | Recall: 0.9127\n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.070\n",
      "\tTest Loss: 0.315\n",
      "\tAccuracy: 0.9043 | F1-Score: 0.9026\n",
      "\tPrecision: 0.9044 | Recall: 0.9008\n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.046\n",
      "\tTest Loss: 0.348\n",
      "\tAccuracy: 0.9121 | F1-Score: 0.9112\n",
      "\tPrecision: 0.9059 | Recall: 0.9167\n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.034\n",
      "\tTest Loss: 0.388\n",
      "\tAccuracy: 0.9160 | F1-Score: 0.9152\n",
      "\tPrecision: 0.9098 | Recall: 0.9206\n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.031\n",
      "\tTest Loss: 0.406\n",
      "\tAccuracy: 0.9238 | F1-Score: 0.9212\n",
      "\tPrecision: 0.9383 | Recall: 0.9048\n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.017\n",
      "\tTest Loss: 0.435\n",
      "\tAccuracy: 0.9277 | F1-Score: 0.9259\n",
      "\tPrecision: 0.9352 | Recall: 0.9167\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, train_iterator)\n",
    "    test_loss, accuracy, f1, precision, recall = evaluate(model, test_iterator)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\tTest Loss: {test_loss:.3f}')\n",
    "    print(f'\\tAccuracy: {accuracy:.4f} | F1-Score: {f1:.4f}')\n",
    "    print(f'\\tPrecision: {precision:.4f} | Recall: {recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ee1e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dece720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da131100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
